{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXUTcl7exA1v",
        "outputId": "04bce2ec-57db-41e2-ea17-49bc33524088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Setting environment variables**\n",
        "\n",
        "To use Hugging Face as a provider for our models, we can create an account and API keys at\n",
        "https://huggingface.co/settings/profile. Additionally, we can make the token available\n",
        "in our environment as HUGGINGFACEHUB_API_TOKEN."
      ],
      "metadata": {
        "id": "CF3P4UvtjrW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/colab_files/langchain_test')\n",
        "\n",
        "from test_lang_chain_key import hugging_face_key\n",
        "import os\n",
        "#os.environ['OPEN_AI_API_KEY'] = openai_key\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = hugging_face_key\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LGUFs3rxSjc",
        "outputId": "bcb12a30-612f-4608-b127-119c70c82a47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 4.21 ms, sys: 1.35 ms, total: 5.56 ms\n",
            "Wall time: 491 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Installing Langchain**"
      ],
      "metadata": {
        "id": "9Q_Y9syVjYEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "GEnb7wXsygPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "m9VUM6t4oH09",
        "outputId": "110e0ef3-ca2d-418d-d647-c261554ed3a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.1.14'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "llm = HuggingFaceHub(\n",
        "model_kwargs={\"temperature\": 0.5, \"max_length\": 64},\n",
        "repo_id=\"google/flan-t5-xxl\"\n",
        ")\n",
        "#temperature establish, from 0 to 1, how creative\n",
        "#model's answers will be\n",
        "\n",
        "#Now we can ask some questions to the model\n",
        "prompt = \"In which country is Tokyo?\"\n",
        "completion = llm.invoke(prompt)\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1HBH3TWhEJp",
        "outputId": "ae0ee330-2233-4de2-a8f2-446b0cbaaf51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "japan\n",
            "CPU times: user 11.3 ms, sys: 0 ns, total: 11.3 ms\n",
            "Wall time: 328 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "prompt = 'I want to open an Italian restaurant. Please suggest me some fancy names.'\n",
        "completion = llm.invoke(prompt)\n",
        "print(completion)"
      ],
      "metadata": {
        "id": "Ob5Dvz1_1OYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6199a566-dcc7-43b2-f4fa-3d27e2ce6f5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Italian Place\n",
            "CPU times: user 6.87 ms, sys: 0 ns, total: 6.87 ms\n",
            "Wall time: 30.9 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "######**Using prompt templates**"
      ],
      "metadata": {
        "id": "GD1zPiN8n0-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm, verbose=True)\n",
        "question = \"What NFL team won the Super Bowl in 2020?\"\n",
        "llm_chain.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u3BGZGmnhuD",
        "outputId": "40f5ef50-3c55-413a-c02e-9e7f05495348"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mQuestion: What NFL team won the Super Bowl in 2020?\n",
            "Answer: Let's think step by step.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'What NFL team won the Super Bowl in 2020?',\n",
              " 'text': '2020 was the 62nd Super Bowl. The New England Patriots won the Super Bowl in 2020. The answer: the New England Patriots.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zn0VNNuTom_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}